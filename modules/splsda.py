import os, shutil, subprocess, gzip
import numpy as np
from collections import Counter

from .parsing import read_gz_file
from .ncls import WindowedNCLS

def validate_r_exists():
    if not shutil.which("R"):
        raise FileNotFoundError("R not found in PATH")
    if not shutil.which("Rscript"):
        raise FileNotFoundError("Rscript not found in PATH")

def validate_r_package(packageName):
    '''
    Checks if the specified R package is installed via command-line
    input to Rscript.
    
    Parameters:
        packageName -- a string indicating the name of the R package to check
    Returns:
        isInstalled -- a boolean indicating whether the package is installed
    '''
    # Format command
    cmd = ["echo", f'\'find.package("{packageName}")\'', "|", "Rscript", "-"]
    
    # Check if package is installed through Rscript interface
    run_Rscript = subprocess.Popen(" ".join(cmd), shell=True,
                                   stdout = subprocess.PIPE,
                                   stderr = subprocess.PIPE)
    rout, rerr = run_Rscript.communicate()
    
    # Check for errors
    errorMsg = rerr.decode("utf-8")
    if "there is no package" in errorMsg or "error" in errorMsg:
        return False
    else:
        # If the package is installed, rout will contain the path to the package
        return True if rout.decode("utf-8").strip() else False

def validate_r_packages_installation():
    '''
    Sequentially checks if the required R packages for sPLS-DA are installed.
    Raises an exception if any package is not installed.
    '''
    REQUIRED_PACKAGES = ["argparser", "mixOmics"]
    for package in REQUIRED_PACKAGES:
        if not validate_r_package(package):
            raise FileNotFoundError(f"The R package '{package}' is not installed. "
                                    "Please install it before running sPLS-DA.")

def recode_vcf(vcfFile, outputFileName):
    '''
    Recode a VCF file to a format suitable for sPLS-DA analysis. Genotypes are encoded as
    an integer (0, 1, or 2) based on the number of minor alleles present. The output file
    is a TSV file with the following format:
    [chrom, pos, sample1EncodedGT, sample2EncodedGT, ...]
    
    Parameters:
        vcfFile -- a string indicating the location of the input VCF file; can be gzipped
        outputFileName -- a string indicating the location of the output file; will be gzipped
    '''
    with read_gz_file(vcfFile) as fileIn, gzip.open(outputFileName, "wt") as fileOut:
        for line in fileIn:
            sl = line.strip().split("\t")
            
            # Handle #CHROM line
            if line.startswith("#CHROM"):
                fileOut.write("\t".join(["chrom", "pos"] + sl[9:]) + "\n")
                continue
            
            # Skip comment lines
            if line.startswith("#"):
                continue
            
            # Skip multiallelic lines
            if "," in sl[4]:
                continue
            
            # Identify genotype position
            gtIndex = sl[8].split(":").index("GT")
            
            # Count the number of alleles to determine major allele
            alleles = Counter([
                allele
                for sampleData in sl[9:]
                for allele in sampleData.split(":")[gtIndex].split("/")
                if allele != "."
            ])
            mostCommonAlleles = alleles.most_common()
            majorAlleles = [
                allele
                for allele, count in mostCommonAlleles
                if count == mostCommonAlleles[0][1]
            ]
            
            # Encode genotype as the number of minor alleles
            encodedLine = [sl[0], sl[1]]
            for sampleData in sl[9:]:
                gt = sampleData.split(":")[gtIndex]
                encodedGT = 0
                if "." in gt:
                    encodedGT = "."
                else:
                    encodedGT += sum([ 1 for allele in gt.split("/") if not allele in majorAlleles ])
                encodedLine.append(str(encodedGT))
            
            # Write to output file
            fileOut.write("\t".join(encodedLine) + "\n")

def run_windowed_splsda(metadataFile, encodedVcfFile, outputVariants, outputBER, outputRdata,
                        scriptLocation, threads=1, windowSize=1000000, berCutoff=0.4, maf=0.05,
                        nrepeat=10, maxiters=1000):
    '''
    Calls the windowed_plsda.R script to run sPLS-DA on the provided encoded VCF file.
    
    Parameters:
        metadataFile -- a string indicating the location of the metadata file; should be a TSV file
                        with columns: [sampleID, bulkGroup] with NO header
        encodedVcfFile -- a string indicating the location of the encoded VCF file 
                          (i.e., a file generated by recode_vcf)
        outputVariants -- a string indicating the location of the output variants file
        outputBER -- a string indicating the location of the output BER file
        outputRdata -- a string indicating the location of the output RData file
        scriptLocation -- a string indicating the location of the windowed_plsda.R script
        threads -- (OPTIONAL) an integer indicating the number of threads to use (default is 1)
        windowSize -- (OPTIONAL) an integer indicating the size of the windows to run local
                       PLS-DA within (default is 1000000)
        berCutoff -- (OPTIONAL) a float indicating the BER cutoff to filter on (default is 0.4)
        maf -- (OPTIONAL) a float indicating the minor allele frequency threshold to
               filter on (default is 0.05)
        nrepeat -- (OPTIONAL) an integer indicating the number of repeats for stability analysis
        maxiters -- (OPTIONAL) an integer indicating the maximum number of iterations when tuning sPLS-DA
    '''
    # Format command
    cmd = ["Rscript", scriptLocation, metadataFile, encodedVcfFile,
           outputVariants, outputBER, outputRdata,
           "--threads", str(threads), "--windowSize", str(windowSize),
           "--berCutoff", str(berCutoff), "--MAF", str(maf),
            "--nrepeat", str(nrepeat), "--maxiters", str(maxiters)]
    
    # Run bcftools index
    run_Rscript = subprocess.Popen(" ".join(cmd), shell=True,
                                   stdout = subprocess.DEVNULL,
                                   stderr = subprocess.PIPE)
    rout, rerr = run_Rscript.communicate()
    
    # Check for errors
    if run_Rscript.returncode == 0:
        return None
    else:
        errorMsg = rerr.decode("utf-8").rstrip("\r\n ")
        raise Exception(("run_windowed_splsda encountered an unhandled situation when processing " + 
                         f"'{encodedVcfFile}'; have a look at the stderr to make sense of this:\n'{errorMsg}'"))

def run_integrative_splsda(callRdataFile, depthRdataFile, outputVariants,
                           scriptLocation, threads=1, nrepeat=10, maxiters=1000):
    '''
    Calls the integrative_splsda.R script to run sPLS-DA on the outputs of call and depth sPLS-DA.
    
    Parameters:
        callRdataFile -- a string indicating the location of the call sPLS-DA RData file
        depthRdataFile -- a string indicating the location of the depth sPLS-DA RData file
        outputVariants -- a string indicating the location of the output variants file
        scriptLocation -- a string indicating the location of the integrative_plsda.R script
        windowSize -- (OPTIONAL) an integer indicating the size of the windows to run local
                       PLS-DA within (default is 1000000)
        berCutoff -- (OPTIONAL) a float indicating the BER cutoff to filter on (default is 0.4)
        maf -- (OPTIONAL) a float indicating the minor allele frequency threshold to
               filter on (default is 0.05)
    '''
    # Format command
    cmd = ["Rscript", scriptLocation, callRdataFile, depthRdataFile, outputVariants,
           "--threads", str(threads), "--nrepeat", str(nrepeat), "--maxiters", str(maxiters)]
    
    # Run bcftools index
    run_Rscript = subprocess.Popen(" ".join(cmd), shell=True,
                                   stdout = subprocess.DEVNULL,
                                   stderr = subprocess.PIPE)
    rout, rerr = run_Rscript.communicate()
    
    # Check for errors
    if run_Rscript.returncode == 0:
        return None
    else:
        errorMsg = rerr.decode("utf-8").rstrip("\r\n ")
        raise Exception(("run_integrative_splsda encountered an unhandled situation when processing " + 
                         f"'{callRdataFile}' and '{depthRdataFile}'; have a look at the stderr to " + 
                         f"make sense of this:\n'{errorMsg}'"))

def parse_selected_to_windowed_ncls(selectedFileName):
    '''
    Parameters:
        selectedFileName -- a file name indicating the location of the selected variants file
    Returns:
        windowedNCLS -- a WindowedNCLS object containing statistical values indexed by chromosome
                        and position
    '''
    EXPECTED_HEADER = ["chrom", "pos", "stability", "abs_loading", "direction"]
    
    # Parse the selected file
    statDict = {}
    with open(selectedFileName, "r") as fileIn:
        # Read and validate the header
        header = fileIn.readline().strip().split("\t")
        if header != EXPECTED_HEADER:
            raise ValueError(f"Invalid header in file '{selectedFileName}', should be: {EXPECTED_HEADER}")
        
        # Store each line in the windowedNCLS object
        for line in fileIn:
            # Parse relevant details
            chrom, pos, stability, abs_loading, direction = line.strip().split("\t")
            try:
                pos = int(float(pos))
            except:
                raise ValueError(f"Position '{pos}' is not an integer in file '{selectedFileName}'")
            try:
                stability = float(stability)
            except:
                raise ValueError(f"Stability '{stability}' is not a float in file '{selectedFileName}'")
            try:
                abs_loading = float(abs_loading)
            except:
                raise ValueError(f"abs_loading '{abs_loading}' is not a float in file '{selectedFileName}'")
            
            # Compute the stability*abs_loading value
            statProduct = stability * abs_loading
            
            # Store the values in the dictionary
            if chrom not in statDict:
                statDict[chrom] = [[], []]
            statDict[chrom][0].append(pos)
            statDict[chrom][1].append(statProduct)
    
    # Convert the dictionary to a WindowedNCLS object
    windowedNCLS = WindowedNCLS(windowSize=0)
    for chrom, value in statDict.items():
        positions = np.array(value[0])
        statsValues = np.array(value[1])
        windowedNCLS.add(chrom, positions, statsValues)
    
    return windowedNCLS

def parse_integrated_to_windowed_ncls(selectedFileName):
    '''
    Parameters:
        selectedFileName -- a file name indicating the location of the selected variants file
    Returns:
        nclsList -- a list of two WindowedNCLS objects:
            callWindowedNCLS -- a WindowedNCLS object containing statistical values indexed by chromosome
                                and position specifically for call variants that were selected
            depthWindowedNCLS -- a WindowedNCLS object containing statistical values indexed by chromosome
                                and position specifically for depth variants that were selected
    '''
    EXPECTED_HEADER = ["chrom", "pos", "type", "stability", "abs_loading", "direction"]
    
    # Parse the selected file
    featureDict = {}
    with open(selectedFileName, "r") as fileIn:
        # Read and validate the header
        header = fileIn.readline().strip().split("\t")
        if header != EXPECTED_HEADER:
            raise ValueError(f"Invalid header in file '{selectedFileName}', should be: {EXPECTED_HEADER}")
        
        # Store each line in the windowedNCLS object
        for line in fileIn:
            # Parse relevant details
            chrom, pos, featuretype, stability, abs_loading, direction = line.strip().split("\t")
            try:
                pos = int(float(pos))
            except:
                raise ValueError(f"Position '{pos}' is not an integer in file '{selectedFileName}'")
            try:
                stability = float(stability)
            except:
                raise ValueError(f"Stability '{stability}' is not a float in file '{selectedFileName}'")
            try:
                abs_loading = float(abs_loading)
            except:
                raise ValueError(f"abs_loading '{abs_loading}' is not a float in file '{selectedFileName}'")
            
            # Compute the stability*abs_loading value
            statProduct = stability * abs_loading
            
            # Store the values in the dictionary
            featureDict.setdefault(featuretype, {})
            
            if chrom not in featureDict[featuretype]:
                featureDict[featuretype][chrom] = [[], []]
            featureDict[featuretype][chrom][0].append(pos)
            featureDict[featuretype][chrom][1].append(statProduct)
    
    # Convert the dictionary to a WindowedNCLS object
    nclsList = []
    for featuretype in ["call", "depth"]: # ensure ordering; we always have at least one of each type
        statDict = featureDict[featuretype]
        windowedNCLS = WindowedNCLS(windowSize=0)
        for chrom, value in statDict.items():
            positions = np.array(value[0])
            statsValues = np.array(value[1])
            windowedNCLS.add(chrom, positions, statsValues)
        nclsList.append(windowedNCLS)
    
    return nclsList

def parse_ber_to_windowed_ncls(berFileName, balancedAccuracy=True):
    '''
    Parameters:
        berFileName -- a file name indicating the location of the BER windows file
        balancedAccuracy -- (OPTIONAL); a boolean indicating whether to convert the
                            BER to balanced accuracy (default is True)
    Returns:
        windowedNCLS -- a WindowedNCLS object containing statistical values indexed
                        by chromosome and position
    '''
    EXPECTED_HEADER = ["chrom", "pos", "BER"]
    
    # Parse the selected file
    statDict = {}
    with open(berFileName, "r") as fileIn:
        # Read and validate the header
        header = fileIn.readline().strip().split("\t")
        if header != EXPECTED_HEADER:
            raise ValueError(f"Invalid header in file '{berFileName}', should be: {EXPECTED_HEADER}")
        
        # Store each line in the windowedNCLS object
        windowSize = None
        prevPos = None
        for line in fileIn:
            # Parse relevant details
            chrom, pos, ber = line.strip().split("\t")
            try:
                pos = int(float(pos))
            except:
                raise ValueError(f"Position '{pos}' is not an integer in file '{berFileName}'")
            try:
                ber = float(ber)
            except:
                raise ValueError(f"BER '{ber}' is not a float in file '{berFileName}'")
            
            # Convert BER to balanced accuracy if needed
            if balancedAccuracy:
                ber = 1 - (ber*2)
            
            # Figure out the window size (if not set yet)
            if windowSize == None:
                if prevPos == None:
                    prevPos = pos
                else:
                    windowSize = pos - prevPos
            
            # Store the values in the dictionary
            if chrom not in statDict:
                statDict[chrom] = [[], []]
            statDict[chrom][0].append(pos)
            statDict[chrom][1].append(ber)
    
    # Convert the dictionary to a WindowedNCLS object
    windowedNCLS = WindowedNCLS(windowSize=windowSize)
    for chrom, value in statDict.items():
        positions = np.array(value[0])
        statsValues = np.array(value[1])
        windowedNCLS.add(chrom, positions, statsValues)
    
    return windowedNCLS, windowSize
